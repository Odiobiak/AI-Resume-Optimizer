{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Odiobiak/AI-Resume-Optimizer/blob/main/Prototype.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e3TwiXGvdsYA"
      },
      "source": [
        "# Adding Web Search\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "3h0ZQ9CrODTW",
        "outputId": "dcbf1444-99bf-455a-c1fb-cdb61bb42324"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: google-generativeai in /usr/local/lib/python3.11/dist-packages (0.8.4)\n",
            "Collecting python-docx\n",
            "  Downloading python_docx-1.1.2-py3-none-any.whl.metadata (2.0 kB)\n",
            "Collecting PyPDF2\n",
            "  Downloading pypdf2-3.0.1-py3-none-any.whl.metadata (6.8 kB)\n",
            "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.11/dist-packages (4.13.4)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (2.32.3)\n",
            "Requirement already satisfied: google-ai-generativelanguage==0.6.15 in /usr/local/lib/python3.11/dist-packages (from google-generativeai) (0.6.15)\n",
            "Requirement already satisfied: google-api-core in /usr/local/lib/python3.11/dist-packages (from google-generativeai) (2.24.2)\n",
            "Requirement already satisfied: google-api-python-client in /usr/local/lib/python3.11/dist-packages (from google-generativeai) (2.164.0)\n",
            "Requirement already satisfied: google-auth>=2.15.0 in /usr/local/lib/python3.11/dist-packages (from google-generativeai) (2.38.0)\n",
            "Requirement already satisfied: protobuf in /usr/local/lib/python3.11/dist-packages (from google-generativeai) (5.29.4)\n",
            "Requirement already satisfied: pydantic in /usr/local/lib/python3.11/dist-packages (from google-generativeai) (2.11.3)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from google-generativeai) (4.67.1)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.11/dist-packages (from google-generativeai) (4.13.2)\n",
            "Requirement already satisfied: proto-plus<2.0.0dev,>=1.22.3 in /usr/local/lib/python3.11/dist-packages (from google-ai-generativelanguage==0.6.15->google-generativeai) (1.26.1)\n",
            "Requirement already satisfied: lxml>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from python-docx) (5.3.2)\n",
            "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.11/dist-packages (from beautifulsoup4) (2.6)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests) (2025.1.31)\n",
            "Requirement already satisfied: googleapis-common-protos<2.0.0,>=1.56.2 in /usr/local/lib/python3.11/dist-packages (from google-api-core->google-generativeai) (1.70.0)\n",
            "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from google-auth>=2.15.0->google-generativeai) (5.5.2)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.11/dist-packages (from google-auth>=2.15.0->google-generativeai) (0.4.2)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.11/dist-packages (from google-auth>=2.15.0->google-generativeai) (4.9.1)\n",
            "Requirement already satisfied: httplib2<1.dev0,>=0.19.0 in /usr/local/lib/python3.11/dist-packages (from google-api-python-client->google-generativeai) (0.22.0)\n",
            "Requirement already satisfied: google-auth-httplib2<1.0.0,>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from google-api-python-client->google-generativeai) (0.2.0)\n",
            "Requirement already satisfied: uritemplate<5,>=3.0.1 in /usr/local/lib/python3.11/dist-packages (from google-api-python-client->google-generativeai) (4.1.1)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic->google-generativeai) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.1 in /usr/local/lib/python3.11/dist-packages (from pydantic->google-generativeai) (2.33.1)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from pydantic->google-generativeai) (0.4.0)\n",
            "Requirement already satisfied: grpcio<2.0dev,>=1.33.2 in /usr/local/lib/python3.11/dist-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0dev,>=1.34.1->google-ai-generativelanguage==0.6.15->google-generativeai) (1.71.0)\n",
            "Requirement already satisfied: grpcio-status<2.0.dev0,>=1.33.2 in /usr/local/lib/python3.11/dist-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0dev,>=1.34.1->google-ai-generativelanguage==0.6.15->google-generativeai) (1.71.0)\n",
            "Requirement already satisfied: pyparsing!=3.0.0,!=3.0.1,!=3.0.2,!=3.0.3,<4,>=2.4.2 in /usr/local/lib/python3.11/dist-packages (from httplib2<1.dev0,>=0.19.0->google-api-python-client->google-generativeai) (3.2.3)\n",
            "Requirement already satisfied: pyasn1<0.7.0,>=0.6.1 in /usr/local/lib/python3.11/dist-packages (from pyasn1-modules>=0.2.1->google-auth>=2.15.0->google-generativeai) (0.6.1)\n",
            "Downloading python_docx-1.1.2-py3-none-any.whl (244 kB)\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m244.3/244.3 kB\u001b[0m \u001b[31m5.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pypdf2-3.0.1-py3-none-any.whl (232 kB)\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m232.6/232.6 kB\u001b[0m \u001b[31m11.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: python-docx, PyPDF2\n",
            "Successfully installed PyPDF2-3.0.1 python-docx-1.1.2\n"
          ]
        }
      ],
      "source": [
        "# =============================================\n",
        "# üîπ Step 1: Install Required Libraries\n",
        "# =============================================\n",
        "!pip install google-generativeai python-docx PyPDF2 beautifulsoup4 requests\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KTepaMSFAdE_"
      },
      "outputs": [],
      "source": [
        "# =============================================\n",
        "# üîπ Step 2: Import Libraries\n",
        "# =============================================\n",
        "import os\n",
        "import re\n",
        "import requests\n",
        "from PyPDF2 import PdfReader\n",
        "from docx import Document\n",
        "from bs4 import BeautifulSoup\n",
        "import google.generativeai as genai\n",
        "from google.colab import files"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SwrOxerFAhi3"
      },
      "outputs": [],
      "source": [
        "# =============================================\n",
        "# üîπ Step 3: Set Up Gemini API\n",
        "# =============================================\n",
        "# Replace with your actual Gemini API key\n",
        "gemini_api_key = \"AIzaSyB8E80w-rqyEB-lDyLq1kYdsFy4Thehkv4\"\n",
        "\n",
        "genai.configure(api_key=gemini_api_key)\n",
        "model = genai.GenerativeModel('gemini-2.0-flash-001')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 90
        },
        "id": "mAhaZSovAmRy",
        "outputId": "1629fb91-906f-46e3-d44c-b71c33ec6eab"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " Please upload your resume file (.pdf, .docx, or .txt)\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-4cb86f35-3e7e-41be-835a-6cb18b189106\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-4cb86f35-3e7e-41be-835a-6cb18b189106\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script>// Copyright 2017 Google LLC\n",
              "//\n",
              "// Licensed under the Apache License, Version 2.0 (the \"License\");\n",
              "// you may not use this file except in compliance with the License.\n",
              "// You may obtain a copy of the License at\n",
              "//\n",
              "//      http://www.apache.org/licenses/LICENSE-2.0\n",
              "//\n",
              "// Unless required by applicable law or agreed to in writing, software\n",
              "// distributed under the License is distributed on an \"AS IS\" BASIS,\n",
              "// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
              "// See the License for the specific language governing permissions and\n",
              "// limitations under the License.\n",
              "\n",
              "/**\n",
              " * @fileoverview Helpers for google.colab Python module.\n",
              " */\n",
              "(function(scope) {\n",
              "function span(text, styleAttributes = {}) {\n",
              "  const element = document.createElement('span');\n",
              "  element.textContent = text;\n",
              "  for (const key of Object.keys(styleAttributes)) {\n",
              "    element.style[key] = styleAttributes[key];\n",
              "  }\n",
              "  return element;\n",
              "}\n",
              "\n",
              "// Max number of bytes which will be uploaded at a time.\n",
              "const MAX_PAYLOAD_SIZE = 100 * 1024;\n",
              "\n",
              "function _uploadFiles(inputId, outputId) {\n",
              "  const steps = uploadFilesStep(inputId, outputId);\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  // Cache steps on the outputElement to make it available for the next call\n",
              "  // to uploadFilesContinue from Python.\n",
              "  outputElement.steps = steps;\n",
              "\n",
              "  return _uploadFilesContinue(outputId);\n",
              "}\n",
              "\n",
              "// This is roughly an async generator (not supported in the browser yet),\n",
              "// where there are multiple asynchronous steps and the Python side is going\n",
              "// to poll for completion of each step.\n",
              "// This uses a Promise to block the python side on completion of each step,\n",
              "// then passes the result of the previous step as the input to the next step.\n",
              "function _uploadFilesContinue(outputId) {\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  const steps = outputElement.steps;\n",
              "\n",
              "  const next = steps.next(outputElement.lastPromiseValue);\n",
              "  return Promise.resolve(next.value.promise).then((value) => {\n",
              "    // Cache the last promise value to make it available to the next\n",
              "    // step of the generator.\n",
              "    outputElement.lastPromiseValue = value;\n",
              "    return next.value.response;\n",
              "  });\n",
              "}\n",
              "\n",
              "/**\n",
              " * Generator function which is called between each async step of the upload\n",
              " * process.\n",
              " * @param {string} inputId Element ID of the input file picker element.\n",
              " * @param {string} outputId Element ID of the output display.\n",
              " * @return {!Iterable<!Object>} Iterable of next steps.\n",
              " */\n",
              "function* uploadFilesStep(inputId, outputId) {\n",
              "  const inputElement = document.getElementById(inputId);\n",
              "  inputElement.disabled = false;\n",
              "\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  outputElement.innerHTML = '';\n",
              "\n",
              "  const pickedPromise = new Promise((resolve) => {\n",
              "    inputElement.addEventListener('change', (e) => {\n",
              "      resolve(e.target.files);\n",
              "    });\n",
              "  });\n",
              "\n",
              "  const cancel = document.createElement('button');\n",
              "  inputElement.parentElement.appendChild(cancel);\n",
              "  cancel.textContent = 'Cancel upload';\n",
              "  const cancelPromise = new Promise((resolve) => {\n",
              "    cancel.onclick = () => {\n",
              "      resolve(null);\n",
              "    };\n",
              "  });\n",
              "\n",
              "  // Wait for the user to pick the files.\n",
              "  const files = yield {\n",
              "    promise: Promise.race([pickedPromise, cancelPromise]),\n",
              "    response: {\n",
              "      action: 'starting',\n",
              "    }\n",
              "  };\n",
              "\n",
              "  cancel.remove();\n",
              "\n",
              "  // Disable the input element since further picks are not allowed.\n",
              "  inputElement.disabled = true;\n",
              "\n",
              "  if (!files) {\n",
              "    return {\n",
              "      response: {\n",
              "        action: 'complete',\n",
              "      }\n",
              "    };\n",
              "  }\n",
              "\n",
              "  for (const file of files) {\n",
              "    const li = document.createElement('li');\n",
              "    li.append(span(file.name, {fontWeight: 'bold'}));\n",
              "    li.append(span(\n",
              "        `(${file.type || 'n/a'}) - ${file.size} bytes, ` +\n",
              "        `last modified: ${\n",
              "            file.lastModifiedDate ? file.lastModifiedDate.toLocaleDateString() :\n",
              "                                    'n/a'} - `));\n",
              "    const percent = span('0% done');\n",
              "    li.appendChild(percent);\n",
              "\n",
              "    outputElement.appendChild(li);\n",
              "\n",
              "    const fileDataPromise = new Promise((resolve) => {\n",
              "      const reader = new FileReader();\n",
              "      reader.onload = (e) => {\n",
              "        resolve(e.target.result);\n",
              "      };\n",
              "      reader.readAsArrayBuffer(file);\n",
              "    });\n",
              "    // Wait for the data to be ready.\n",
              "    let fileData = yield {\n",
              "      promise: fileDataPromise,\n",
              "      response: {\n",
              "        action: 'continue',\n",
              "      }\n",
              "    };\n",
              "\n",
              "    // Use a chunked sending to avoid message size limits. See b/62115660.\n",
              "    let position = 0;\n",
              "    do {\n",
              "      const length = Math.min(fileData.byteLength - position, MAX_PAYLOAD_SIZE);\n",
              "      const chunk = new Uint8Array(fileData, position, length);\n",
              "      position += length;\n",
              "\n",
              "      const base64 = btoa(String.fromCharCode.apply(null, chunk));\n",
              "      yield {\n",
              "        response: {\n",
              "          action: 'append',\n",
              "          file: file.name,\n",
              "          data: base64,\n",
              "        },\n",
              "      };\n",
              "\n",
              "      let percentDone = fileData.byteLength === 0 ?\n",
              "          100 :\n",
              "          Math.round((position / fileData.byteLength) * 100);\n",
              "      percent.textContent = `${percentDone}% done`;\n",
              "\n",
              "    } while (position < fileData.byteLength);\n",
              "  }\n",
              "\n",
              "  // All done.\n",
              "  yield {\n",
              "    response: {\n",
              "      action: 'complete',\n",
              "    }\n",
              "  };\n",
              "}\n",
              "\n",
              "scope.google = scope.google || {};\n",
              "scope.google.colab = scope.google.colab || {};\n",
              "scope.google.colab._files = {\n",
              "  _uploadFiles,\n",
              "  _uploadFilesContinue,\n",
              "};\n",
              "})(self);\n",
              "</script> "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Saving Resume_Oliver_Lee.docx to Resume_Oliver_Lee.docx\n"
          ]
        }
      ],
      "source": [
        "# =============================================\n",
        "# üîπ Step 4: Upload Resume File\n",
        "# =============================================\n",
        "print(\" Please upload your resume file (.pdf, .docx, or .txt)\")\n",
        "uploaded = files.upload()\n",
        "resume_path = list(uploaded.keys())[0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HmnOakm_ApDX",
        "outputId": "47b74b94-5df2-4b96-c3fe-43fbb5a2674e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " Resume loaded.\n"
          ]
        }
      ],
      "source": [
        "# =============================================\n",
        "# üîπ Step 5: Read Resume Content\n",
        "# =============================================\n",
        "def read_file(file_path):\n",
        "    if file_path.lower().endswith(\".txt\"):\n",
        "        with open(file_path, 'r', encoding='utf-8') as file:\n",
        "            return file.read()\n",
        "    elif file_path.lower().endswith(\".pdf\"):\n",
        "        text = \"\"\n",
        "        with open(file_path, 'rb') as file:\n",
        "            reader = PdfReader(file)\n",
        "            for page in reader.pages:\n",
        "                text += page.extract_text() or \"\"\n",
        "        return text\n",
        "    elif file_path.lower().endswith(\".docx\"):\n",
        "        doc = Document(file_path)\n",
        "        return \"\\n\".join([p.text for p in doc.paragraphs])\n",
        "    else:\n",
        "        return \" Unsupported file type\"\n",
        "\n",
        "resume_text = read_file(resume_path)\n",
        "print(\" Resume loaded.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "id": "KKyYbpjUAv54",
        "outputId": "5134c462-4c2d-41fa-e383-a5034181e692"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Gemini summary:\n",
            "Search for jobs related to: data analysis, data science, business intelligence, data visualization, project management, and machine learning.\n"
          ]
        }
      ],
      "source": [
        "# =============================================\n",
        "# üîπ Step 6: Ask Gemini to Summarize Resume\n",
        "# =============================================\n",
        "def summarize_resume(text):\n",
        "    prompt = f\"\"\"\n",
        "    You are an AI assistant. Analyze the resume below and extract a list of key skills, job titles, and fields of expertise.\n",
        "\n",
        "    Resume:\n",
        "    {text}\n",
        "\n",
        "    Respond with a short sentence like:\n",
        "    \"Search for jobs related to: data analysis, Python, business intelligence, data science, data visulization, SQL\"\n",
        "    \"\"\"\n",
        "    response = model.generate_content(prompt)\n",
        "    return response.text.strip()\n",
        "\n",
        "summary = summarize_resume(resume_text)\n",
        "print(\"Gemini summary:\")\n",
        "print(summary)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Lz1552s8BB9N",
        "outputId": "3bb1e6e0-23d2-4a8f-b68b-e2242325548c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            " Searching jobs for: Data Analyst\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# =============================================\n",
        "# üîπ Extract job title/keywords for search\n",
        "# =============================================\n",
        "match = re.findall(r\"\\b(?:data analyst|software engineer|developer|data scientist|project manager)\\b\", summary.lower())\n",
        "search_term = match[0] if match else \"data analyst\"\n",
        "print(f\"\\n Searching jobs for: {search_term.title()}\\n\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8TJJHfSrBbO6",
        "outputId": "3c5dbc50-71b4-45e1-9402-fd76d23fe9ac"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Found 10 jobs:\n",
            "\n",
            "Data Analyst Quality & Safety at MEDSTAR HEALTH\n",
            "Washington, District of Columbia\n",
            "https://careers.medstarhealth.org/global/en/job/req43258/Data-Analyst-Quality-Safety?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic\n",
            "\n",
            "Analyst Level 3 at Think Tank Inc.\n",
            "Silver Spring, Maryland\n",
            "https://www.linkedin.com/jobs/view/analyst-level-3-at-think-tank-inc-4212577730?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic\n",
            "\n",
            "Operations Data Analyst, HAP Tech at Berkeley Research Group, LLC\n",
            "Washington, District of Columbia\n",
            "https://www.indeed.com/viewjob?jk=cecffc025f36cebb&utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic\n",
            "\n",
            "Data Analyst Jobs at Technomics, Inc.\n",
            "Arlington, Virginia\n",
            "https://www.clearancejobs.com/jobs/5601704/data-analyst?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic\n",
            "\n",
            "Data Analyst MUST HAVE HASTUS experience at Zenosys\n",
            "Alexandria, Virginia\n",
            "https://www.dice.com/job-detail/9686e56e-3834-4c03-9fdf-98ce1ebc5ed8?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic\n",
            "\n",
            "Data Analyst Supporting the FBI at FSA Federal\n",
            "Washington, District of Columbia\n",
            "https://www.ziprecruiter.com/c/FSA-Federal/Job/Data-Analyst-Supporting-the-FBI/-in-Washington,DC?jid=29ad7564c2221238&utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic\n",
            "\n",
            "Lead Data Analyst, Customer Insights at Arcadia\n",
            "Washington, District of Columbia\n",
            "https://jobs.lever.co/arcadia/8fb2805b-430f-436e-8377-aebcf5f29b65?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic\n",
            "\n",
            "Senior Business Data Analyst at Rose International\n",
            "Washington, District of Columbia\n",
            "https://www.career.com/job/rose-international/senior-business-data-analyst/j202501191817551115786?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic\n",
            "\n",
            "Data Analyst I - Pediatrics at The Henry M. Jackson Foundation for the Advancement of Military Medicine\n",
            "Bethesda, Maryland\n",
            "https://jobs.hjf.org/jobs/5382?lang=en-us&utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic\n",
            "\n",
            "Junior Data Analyst at Noblis\n",
            "Washington, District of Columbia\n",
            "https://www.dataanalyst.com/job/junior-data-analyst-4bac7?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# =============================================\n",
        "# üîπ Search jobs using JSearch API\n",
        "# =============================================\n",
        "def search_jobs_jsearch(query, location=\"\", api_key=\"ec609eab68mshfc1acbdd1705c35p17c933jsn64f7236169db\"):\n",
        "    import requests\n",
        "\n",
        "    url = \"https://jsearch.p.rapidapi.com/search\"\n",
        "\n",
        "    params = {\n",
        "        \"query\": query,\n",
        "        \"page\": \"1\",\n",
        "        \"num_pages\": \"1\",\n",
        "        \"location\": location\n",
        "    }\n",
        "\n",
        "    headers = {\n",
        "        \"X-RapidAPI-Key\": api_key,\n",
        "        \"X-RapidAPI-Host\": \"jsearch.p.rapidapi.com\"\n",
        "    }\n",
        "\n",
        "    response = requests.get(url, headers=headers, params=params)\n",
        "\n",
        "    if response.status_code != 200:\n",
        "        print(\"Error fetching jobs:\", response.status_code)\n",
        "        print(response.text)\n",
        "        return\n",
        "\n",
        "    jobs = response.json().get(\"data\", [])\n",
        "\n",
        "    if not jobs:\n",
        "        print(\"No jobs found.\")\n",
        "        return\n",
        "\n",
        "    print(f\"Found {len(jobs)} jobs:\\n\")\n",
        "\n",
        "    for job in jobs[:10]:\n",
        "        print(f\"{job['job_title']} at {job['employer_name']}\")\n",
        "        print(f\"{job['job_city']}, {job['job_state']}\")\n",
        "        print(f\"{job['job_apply_link']}\\n\")\n",
        "\n",
        "# Call it with the inferred search term\n",
        "search_jobs_jsearch(search_term, location=\"United States\", api_key=\"ec609eab68mshfc1acbdd1705c35p17c933jsn64f7236169db\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EStgEIIhJTXR"
      },
      "source": [
        "\n",
        "\n",
        "```\n",
        "# This is formatted as code\n",
        "```\n",
        "\n",
        "# Resume Optimizer\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "WhZuRZ5SSoQv",
        "outputId": "781bcd48-7870-4845-b9ba-f17241a73ddb"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: google-generativeai in /usr/local/lib/python3.11/dist-packages (0.8.5)\n",
            "Requirement already satisfied: google-ai-generativelanguage==0.6.15 in /usr/local/lib/python3.11/dist-packages (from google-generativeai) (0.6.15)\n",
            "Requirement already satisfied: google-api-core in /usr/local/lib/python3.11/dist-packages (from google-generativeai) (2.24.2)\n",
            "Requirement already satisfied: google-api-python-client in /usr/local/lib/python3.11/dist-packages (from google-generativeai) (2.168.0)\n",
            "Requirement already satisfied: google-auth>=2.15.0 in /usr/local/lib/python3.11/dist-packages (from google-generativeai) (2.38.0)\n",
            "Requirement already satisfied: protobuf in /usr/local/lib/python3.11/dist-packages (from google-generativeai) (5.29.4)\n",
            "Requirement already satisfied: pydantic in /usr/local/lib/python3.11/dist-packages (from google-generativeai) (2.11.3)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from google-generativeai) (4.67.1)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.11/dist-packages (from google-generativeai) (4.13.2)\n",
            "Requirement already satisfied: proto-plus<2.0.0dev,>=1.22.3 in /usr/local/lib/python3.11/dist-packages (from google-ai-generativelanguage==0.6.15->google-generativeai) (1.26.1)\n",
            "Requirement already satisfied: googleapis-common-protos<2.0.0,>=1.56.2 in /usr/local/lib/python3.11/dist-packages (from google-api-core->google-generativeai) (1.70.0)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.18.0 in /usr/local/lib/python3.11/dist-packages (from google-api-core->google-generativeai) (2.32.3)\n",
            "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from google-auth>=2.15.0->google-generativeai) (5.5.2)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.11/dist-packages (from google-auth>=2.15.0->google-generativeai) (0.4.2)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.11/dist-packages (from google-auth>=2.15.0->google-generativeai) (4.9.1)\n",
            "Requirement already satisfied: httplib2<1.0.0,>=0.19.0 in /usr/local/lib/python3.11/dist-packages (from google-api-python-client->google-generativeai) (0.22.0)\n",
            "Requirement already satisfied: google-auth-httplib2<1.0.0,>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from google-api-python-client->google-generativeai) (0.2.0)\n",
            "Requirement already satisfied: uritemplate<5,>=3.0.1 in /usr/local/lib/python3.11/dist-packages (from google-api-python-client->google-generativeai) (4.1.1)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic->google-generativeai) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.1 in /usr/local/lib/python3.11/dist-packages (from pydantic->google-generativeai) (2.33.1)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from pydantic->google-generativeai) (0.4.0)\n",
            "Requirement already satisfied: grpcio<2.0dev,>=1.33.2 in /usr/local/lib/python3.11/dist-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0dev,>=1.34.1->google-ai-generativelanguage==0.6.15->google-generativeai) (1.71.0)\n",
            "Requirement already satisfied: grpcio-status<2.0.dev0,>=1.33.2 in /usr/local/lib/python3.11/dist-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0dev,>=1.34.1->google-ai-generativelanguage==0.6.15->google-generativeai) (1.71.0)\n",
            "Requirement already satisfied: pyparsing!=3.0.0,!=3.0.1,!=3.0.2,!=3.0.3,<4,>=2.4.2 in /usr/local/lib/python3.11/dist-packages (from httplib2<1.0.0,>=0.19.0->google-api-python-client->google-generativeai) (3.2.3)\n",
            "Requirement already satisfied: pyasn1<0.7.0,>=0.6.1 in /usr/local/lib/python3.11/dist-packages (from pyasn1-modules>=0.2.1->google-auth>=2.15.0->google-generativeai) (0.6.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0,>=2.18.0->google-api-core->google-generativeai) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0,>=2.18.0->google-api-core->google-generativeai) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0,>=2.18.0->google-api-core->google-generativeai) (2.4.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0,>=2.18.0->google-api-core->google-generativeai) (2025.1.31)\n",
            "Requirement already satisfied: python-docx in /usr/local/lib/python3.11/dist-packages (1.1.2)\n",
            "Requirement already satisfied: lxml>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from python-docx) (5.4.0)\n",
            "Requirement already satisfied: typing-extensions>=4.9.0 in /usr/local/lib/python3.11/dist-packages (from python-docx) (4.13.2)\n"
          ]
        }
      ],
      "source": [
        "#!pip install google-generativeai python-docx pyPDF2 sklearn\n",
        "!pip install --upgrade google-generativeai\n",
        "!pip install python-docx\n",
        "!pip install PyPDF2\n",
        "!pip install sklearn"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ND847YSxSs18"
      },
      "outputs": [],
      "source": [
        "import google.generativeai as genai\n",
        "import os\n",
        "import re\n",
        "import PyPDF2\n",
        "import docx\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "import textwrap\n",
        "from google.colab import files"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P1kUoUhMnsvi"
      },
      "source": [
        "Setting up the Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kZvfPVZ5SviE"
      },
      "outputs": [],
      "source": [
        "# setting up the model\n",
        "genai.configure(api_key=\"AIzaSyCr7B-Ox86HFFDqodh2p2pruSTrmgN9MD4\")\n",
        "\n",
        "generation_config ={\n",
        "    \"temperature\": 0.5,\n",
        "    \"max_output_tokens\": 8192,\n",
        "    \"top_k\": 40,\n",
        "    \"top_p\": 0.95\n",
        "}\n",
        "\n",
        "\n",
        "# adding safety features\n",
        "safety_settings = [\n",
        "    {\"category\": \"HARM_CATEGORY_HARASSMENT\", \"threshold\": \"BLOCK_NONE\"},\n",
        "    {\"category\": \"HARM_CATEGORY_HATE_SPEECH\", \"threshold\": \"BLOCK_NONE\"},\n",
        "    {\"category\": \"HARM_CATEGORY_SEXUALLY_EXPLICIT\", \"threshold\": \"BLOCK_NONE\"},\n",
        "    {\"category\": \"HARM_CATEGORY_DANGEROUS_CONTENT\", \"threshold\": \"BLOCK_NONE\"},\n",
        "]\n",
        "\n",
        "model = genai.GenerativeModel(\n",
        "    'gemini-2.0-flash-001',\n",
        "    generation_config=generation_config,\n",
        "    safety_settings=safety_settings\n",
        ")\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-Sb8vBAiS0Ja"
      },
      "outputs": [],
      "source": [
        "# Function to extract the text from the uploaded resume, include pdf, word or text format\n",
        "\n",
        "def extract_text_from_pdf(pdf_path):\n",
        "    text = \"\"\n",
        "    with open(pdf_path, 'rb') as file:\n",
        "        pdf_reader = PyPDF2.PdfReader(file)\n",
        "        for page in pdf_reader.pages:\n",
        "            text += page.extract_text()\n",
        "    return text\n",
        "\n",
        "def extract_text_from_docx(docx_path):\n",
        "    doc = docx.Document(docx_path)\n",
        "    full_text = []\n",
        "    for para in doc.paragraphs:\n",
        "        full_text.append(para.text)\n",
        "    return '\\n'.join(full_text)\n",
        "\n",
        "# To extract different versions of the resume\n",
        "def extract_text(file_path):\n",
        "    if file_path.endswith('.pdf'):\n",
        "        return extract_text_from_pdf(file_path)\n",
        "    elif file_path.endswith('.docx'):\n",
        "        return extract_text_from_docx(file_path)\n",
        "    else:\n",
        "        with open(file_path, 'r') as file:\n",
        "            return file.read()\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "p6GFkD7US1Rw"
      },
      "outputs": [],
      "source": [
        "# Using embedding for better similarity match\n",
        "def get_embedding(text):\n",
        "    truncated_text = text[:10000]\n",
        "    result = genai.embed_content(\n",
        "        model=\"models/embedding-001\",\n",
        "        content=truncated_text,\n",
        "        task_type=\"retrieval_document\"\n",
        "    )\n",
        "    return result['embedding']\n",
        "\n",
        "# use cosine to calculate similarity between two texts\n",
        "def calculate_similarity(text1, text2):\n",
        "    emb1 = get_embedding(text1)\n",
        "    emb2 = get_embedding(text2)\n",
        "    return cosine_similarity([emb1], [emb2])[0][0]\n",
        "\n",
        "\n",
        "\n",
        "# analyse and refine the resume\n",
        "# Passing the prompt to Returns structured analysis with: match score, strengths, gaps, and recommendations\n",
        "def analyze_alignment(resume_text, jd_text):\n",
        "\n",
        "    prompt = textwrap.dedent(f\"\"\"\n",
        "    **Resume-Job Description Alignment Analysis**\n",
        "\n",
        "    Analyze how well this resume matches the job description by examining:\n",
        "    - Required skills/technologies\n",
        "    - Years and type of experience\n",
        "    - Education/certifications\n",
        "    - Keywords and terminology\n",
        "    - Soft skills and cultural fit\n",
        "\n",
        "    **Resume Content:**\n",
        "    {resume_text[:10000]}  # Limiting to first 10k chars for Gemini context\n",
        "\n",
        "    **Job Description Content:**\n",
        "    {jd_text[:10000]}\n",
        "\n",
        "    Provide your analysis in this exact format:\n",
        "\n",
        "    ### Match Score: [0-100]%\n",
        "    [Explanation of overall match quality]\n",
        "\n",
        "    ### Key Strengths:\n",
        "    - [Strength 1 with specific evidence] from resume\n",
        "    - [Strength 2 with specific evidence]\n",
        "    - [Strength 3 with specific evidence]\n",
        "\n",
        "    ### Critical Gaps:\n",
        "    - [Missing requirement 1 with impact] (Critical/Important/Nice-to-Have)\n",
        "    - [Missing requirement 2 with impact]\n",
        "    - [Missing requirement 3 with impact]\n",
        "\n",
        "    ### Improvement Recommendations:\n",
        "    - [Actionable suggestion 1] (most impactful)\n",
        "    - [Actionable suggestion 2]\n",
        "    - [Actionable suggestion 3]\n",
        "\n",
        "    ### Keyword Optimization:\n",
        "    [List of important JD keywords missing from resume]\n",
        "    \"\"\")\n",
        "\n",
        "    # Error catch\n",
        "    try:\n",
        "        response = model.generate_content(prompt)\n",
        "        return response.text\n",
        "    except Exception as e:\n",
        "        return f\"Analysis failed: {str(e)}\"\n",
        "\n",
        "# Refine the resume to match the job description.\n",
        "# This Maintains original information while optimizing for the JD\n",
        "\n",
        "def refine_resume(resume_text, jd_text):\n",
        "\n",
        "    prompt = textwrap.dedent(f\"\"\"\n",
        "    **Resume Refinement Task**\n",
        "\n",
        "    You are a professional resume writer. Rewrite this resume to better match the\n",
        "    target job description while:\n",
        "    1. Keeping all factual information 100% accurate\n",
        "    2. Maintaining the original structure and sections\n",
        "    3. Adding missing keywords from the JD naturally\n",
        "    4. Reordering content to highlight relevant experience first\n",
        "    5. Quantifying achievements where possible\n",
        "    6. Rewrite and include experience to match the JD\n",
        "\n",
        "    **Original Resume:**\n",
        "    {resume_text[:10000]}\n",
        "\n",
        "    **Target Job Description:**\n",
        "    {jd_text[:10000]}\n",
        "\n",
        "    **MODIFICATION GUIDELINES:**\n",
        "    - Incorporate at least 3 key terms from the JD\n",
        "    - Add a \"Relevant Skills\" section if missing\n",
        "    - Ensure work experience highlights JD-relevant accomplishments\n",
        "    - Optimize the professional summary for this role\n",
        "\n",
        "    Return ONLY the refined resume text with no additional commentary or analysis.\n",
        "    Maintain the same format as the original (bullet points, sections, etc).\n",
        "    \"\"\")\n",
        "\n",
        "# Error catch\n",
        "\n",
        "    try:\n",
        "        response = model.generate_content(prompt)\n",
        "\n",
        "        # Clean up response to ensure only resume content\n",
        "        refined = response.text\n",
        "        if refined.startswith(\"```\"):\n",
        "            refined = refined[3:]\n",
        "        if refined.endswith(\"```\"):\n",
        "            refined = refined[:-3]\n",
        "        return refined.strip()\n",
        "    except Exception as e:\n",
        "        return f\"Refinement failed: {str(e)}\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2KMg7aRrgONl"
      },
      "source": [
        "Generating Cover Letter"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "97UAVZbu3N1W"
      },
      "outputs": [],
      "source": [
        "def generate_cover_letter(resume_text, jd_text):\n",
        "    # prompt sent to the AI model.It tells the AI towrite a personalized cover letter\n",
        "    # by pulling relevant info from the candidate's resume and the job description.\n",
        "    prompt = textwrap.dedent(f\"\"\"  # Assigning the prompt here\n",
        "        **Generate a cover letter based on the job description**\n",
        "\n",
        "        Resume:\n",
        "        {resume_text}\n",
        "\n",
        "\n",
        "        Job Description:\n",
        "        {jd_text}\n",
        "\n",
        "\n",
        "        Include:\n",
        "        - Show how the skills and experience align with the job description\n",
        "        - Brief summary of relevant experience\n",
        "        - Express interest in the position and company\n",
        "        - Include key skills that match the job\n",
        "        - Passionate closing\n",
        "        Return only the letter.\n",
        "    \"\"\")\n",
        "\n",
        "    #get the generated response\n",
        "    response = model.generate_content(prompt)\n",
        "    return response.text"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "collapsed": true,
        "id": "HOu8QWy6S4fO",
        "outputId": "f6d9703e-9f5e-4961-d7d2-6bc097082fe9"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " Resume Analyzer & Optimization Tool\n",
            "-------------------------------------\n",
            "\n",
            " STEP 1: Upload your resume (PDF/DOCX/TXT):\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-1bc6ad8b-9218-49ea-89d3-1f2def15321e\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-1bc6ad8b-9218-49ea-89d3-1f2def15321e\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script>// Copyright 2017 Google LLC\n",
              "//\n",
              "// Licensed under the Apache License, Version 2.0 (the \"License\");\n",
              "// you may not use this file except in compliance with the License.\n",
              "// You may obtain a copy of the License at\n",
              "//\n",
              "//      http://www.apache.org/licenses/LICENSE-2.0\n",
              "//\n",
              "// Unless required by applicable law or agreed to in writing, software\n",
              "// distributed under the License is distributed on an \"AS IS\" BASIS,\n",
              "// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
              "// See the License for the specific language governing permissions and\n",
              "// limitations under the License.\n",
              "\n",
              "/**\n",
              " * @fileoverview Helpers for google.colab Python module.\n",
              " */\n",
              "(function(scope) {\n",
              "function span(text, styleAttributes = {}) {\n",
              "  const element = document.createElement('span');\n",
              "  element.textContent = text;\n",
              "  for (const key of Object.keys(styleAttributes)) {\n",
              "    element.style[key] = styleAttributes[key];\n",
              "  }\n",
              "  return element;\n",
              "}\n",
              "\n",
              "// Max number of bytes which will be uploaded at a time.\n",
              "const MAX_PAYLOAD_SIZE = 100 * 1024;\n",
              "\n",
              "function _uploadFiles(inputId, outputId) {\n",
              "  const steps = uploadFilesStep(inputId, outputId);\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  // Cache steps on the outputElement to make it available for the next call\n",
              "  // to uploadFilesContinue from Python.\n",
              "  outputElement.steps = steps;\n",
              "\n",
              "  return _uploadFilesContinue(outputId);\n",
              "}\n",
              "\n",
              "// This is roughly an async generator (not supported in the browser yet),\n",
              "// where there are multiple asynchronous steps and the Python side is going\n",
              "// to poll for completion of each step.\n",
              "// This uses a Promise to block the python side on completion of each step,\n",
              "// then passes the result of the previous step as the input to the next step.\n",
              "function _uploadFilesContinue(outputId) {\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  const steps = outputElement.steps;\n",
              "\n",
              "  const next = steps.next(outputElement.lastPromiseValue);\n",
              "  return Promise.resolve(next.value.promise).then((value) => {\n",
              "    // Cache the last promise value to make it available to the next\n",
              "    // step of the generator.\n",
              "    outputElement.lastPromiseValue = value;\n",
              "    return next.value.response;\n",
              "  });\n",
              "}\n",
              "\n",
              "/**\n",
              " * Generator function which is called between each async step of the upload\n",
              " * process.\n",
              " * @param {string} inputId Element ID of the input file picker element.\n",
              " * @param {string} outputId Element ID of the output display.\n",
              " * @return {!Iterable<!Object>} Iterable of next steps.\n",
              " */\n",
              "function* uploadFilesStep(inputId, outputId) {\n",
              "  const inputElement = document.getElementById(inputId);\n",
              "  inputElement.disabled = false;\n",
              "\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  outputElement.innerHTML = '';\n",
              "\n",
              "  const pickedPromise = new Promise((resolve) => {\n",
              "    inputElement.addEventListener('change', (e) => {\n",
              "      resolve(e.target.files);\n",
              "    });\n",
              "  });\n",
              "\n",
              "  const cancel = document.createElement('button');\n",
              "  inputElement.parentElement.appendChild(cancel);\n",
              "  cancel.textContent = 'Cancel upload';\n",
              "  const cancelPromise = new Promise((resolve) => {\n",
              "    cancel.onclick = () => {\n",
              "      resolve(null);\n",
              "    };\n",
              "  });\n",
              "\n",
              "  // Wait for the user to pick the files.\n",
              "  const files = yield {\n",
              "    promise: Promise.race([pickedPromise, cancelPromise]),\n",
              "    response: {\n",
              "      action: 'starting',\n",
              "    }\n",
              "  };\n",
              "\n",
              "  cancel.remove();\n",
              "\n",
              "  // Disable the input element since further picks are not allowed.\n",
              "  inputElement.disabled = true;\n",
              "\n",
              "  if (!files) {\n",
              "    return {\n",
              "      response: {\n",
              "        action: 'complete',\n",
              "      }\n",
              "    };\n",
              "  }\n",
              "\n",
              "  for (const file of files) {\n",
              "    const li = document.createElement('li');\n",
              "    li.append(span(file.name, {fontWeight: 'bold'}));\n",
              "    li.append(span(\n",
              "        `(${file.type || 'n/a'}) - ${file.size} bytes, ` +\n",
              "        `last modified: ${\n",
              "            file.lastModifiedDate ? file.lastModifiedDate.toLocaleDateString() :\n",
              "                                    'n/a'} - `));\n",
              "    const percent = span('0% done');\n",
              "    li.appendChild(percent);\n",
              "\n",
              "    outputElement.appendChild(li);\n",
              "\n",
              "    const fileDataPromise = new Promise((resolve) => {\n",
              "      const reader = new FileReader();\n",
              "      reader.onload = (e) => {\n",
              "        resolve(e.target.result);\n",
              "      };\n",
              "      reader.readAsArrayBuffer(file);\n",
              "    });\n",
              "    // Wait for the data to be ready.\n",
              "    let fileData = yield {\n",
              "      promise: fileDataPromise,\n",
              "      response: {\n",
              "        action: 'continue',\n",
              "      }\n",
              "    };\n",
              "\n",
              "    // Use a chunked sending to avoid message size limits. See b/62115660.\n",
              "    let position = 0;\n",
              "    do {\n",
              "      const length = Math.min(fileData.byteLength - position, MAX_PAYLOAD_SIZE);\n",
              "      const chunk = new Uint8Array(fileData, position, length);\n",
              "      position += length;\n",
              "\n",
              "      const base64 = btoa(String.fromCharCode.apply(null, chunk));\n",
              "      yield {\n",
              "        response: {\n",
              "          action: 'append',\n",
              "          file: file.name,\n",
              "          data: base64,\n",
              "        },\n",
              "      };\n",
              "\n",
              "      let percentDone = fileData.byteLength === 0 ?\n",
              "          100 :\n",
              "          Math.round((position / fileData.byteLength) * 100);\n",
              "      percent.textContent = `${percentDone}% done`;\n",
              "\n",
              "    } while (position < fileData.byteLength);\n",
              "  }\n",
              "\n",
              "  // All done.\n",
              "  yield {\n",
              "    response: {\n",
              "      action: 'complete',\n",
              "    }\n",
              "  };\n",
              "}\n",
              "\n",
              "scope.google = scope.google || {};\n",
              "scope.google.colab = scope.google.colab || {};\n",
              "scope.google.colab._files = {\n",
              "  _uploadFiles,\n",
              "  _uploadFilesContinue,\n",
              "};\n",
              "})(self);\n",
              "</script> "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "ename": "TypeError",
          "evalue": "'NoneType' object is not subscriptable",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-9-6a8c195f4253>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     55\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     56\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"__main__\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 57\u001b[0;31m     \u001b[0mmain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-9-6a8c195f4253>\u001b[0m in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0;31m# File uploads\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"\\n STEP 1: Upload your resume (PDF/DOCX/TXT):\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m     \u001b[0muploaded_resume\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfiles\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m     \u001b[0mresume_file\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0muploaded_resume\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m     \u001b[0mresume_text\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mextract_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresume_file\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/google/colab/files.py\u001b[0m in \u001b[0;36mupload\u001b[0;34m(target_dir)\u001b[0m\n\u001b[1;32m     70\u001b[0m   \"\"\"\n\u001b[1;32m     71\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 72\u001b[0;31m   \u001b[0muploaded_files\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_upload_files\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmultiple\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     73\u001b[0m   \u001b[0;31m# Mapping from original filename to filename as saved locally.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     74\u001b[0m   \u001b[0mlocal_filenames\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/google/colab/files.py\u001b[0m in \u001b[0;36m_upload_files\u001b[0;34m(multiple)\u001b[0m\n\u001b[1;32m    169\u001b[0m   \u001b[0mfiles\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_collections\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdefaultdict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbytes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    170\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 171\u001b[0;31m   \u001b[0;32mwhile\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'action'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;34m'complete'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    172\u001b[0m     result = _output.eval_js(\n\u001b[1;32m    173\u001b[0m         'google.colab._files._uploadFilesContinue(\"{output_id}\")'.format(\n",
            "\u001b[0;31mTypeError\u001b[0m: 'NoneType' object is not subscriptable"
          ]
        }
      ],
      "source": [
        "# Main function\n",
        "def main():\n",
        "    print(\" Resume Analyzer & Optimization Tool\")\n",
        "    print(\"-------------------------------------\")\n",
        "\n",
        "    # File uploads\n",
        "    print(\"\\n STEP 1: Upload your resume (PDF/DOCX/TXT):\")\n",
        "    uploaded_resume = files.upload()\n",
        "    resume_file = next(iter(uploaded_resume))\n",
        "    resume_text = extract_text(resume_file)\n",
        "\n",
        "    print(\"\\n STEP 2: Upload the job description (PDF/DOCX/TXT):\")\n",
        "    uploaded_jd = files.upload()\n",
        "    jd_file = next(iter(uploaded_jd))\n",
        "    jd_text = extract_text(jd_file)\n",
        "\n",
        "    # Calculate embedding similarity\n",
        "    print(\"\\n Calculating Semantic\")\n",
        "    similarity_score = calculate_similarity(resume_text, jd_text)\n",
        "    print(f\"\\n Semantic Match Score: {similarity_score*100:.1f}%\")\n",
        "\n",
        "    # Detailed analysis\n",
        "    print(\"\\n Detailed Analysis:\")\n",
        "    analysis = analyze_alignment(resume_text, jd_text)\n",
        "    print(analysis)\n",
        "\n",
        "    # Resume refinement\n",
        "    print(\"\\n Generating optimized resume...\")\n",
        "    optimized_resume = refine_resume(resume_text, jd_text)\n",
        "\n",
        "    print(\"\\n OPTIMIZED RESUME:\")\n",
        "    print(\"-------------------\")\n",
        "    print(optimized_resume)\n",
        "\n",
        "    # Save and offer download\n",
        "    with open('optimized_resume.txt', 'w') as f:\n",
        "        f.write(optimized_resume)\n",
        "    print(\"\\n Download your optimized resume:\")\n",
        "    files.download('optimized_resume.txt')\n",
        "\n",
        "    # Generate Cover Letter\n",
        "    print(\"\\n Generating cover letter...\")\n",
        "    cover_letter = generate_cover_letter(resume_text, jd_text)\n",
        "    print(\"\\n COVER LETTER:\")\n",
        "    print(\"-------------------\")\n",
        "    print(cover_letter)\n",
        "\n",
        "    # Save the generated cover letter to a text file for download\n",
        "    with open('cover_letter.txt', 'w') as f:\n",
        "        f.write(cover_letter)\n",
        "\n",
        "    # Provide a link for the user to download the saved cover letter\n",
        "    print(\"\\n Download your personalized cover letter:\")\n",
        "    files.download('cover_letter.txt')\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HKYs3g2jFYMJ"
      },
      "source": [
        "# THE PROTOTYPE"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xdGaIIpaFjiu"
      },
      "outputs": [],
      "source": [
        "!pip install google-generativeai python-docx pyPDF2 sklearn\n",
        "#!pip install google-generativeai==0.3.1\n",
        "#!pip install google-genai -U\n",
        "#!pip install PyPDF2\n",
        "!pip install python-docx"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NXHwri5QF6WQ"
      },
      "outputs": [],
      "source": [
        "import google.generativeai as genai\n",
        "import PyPDF2\n",
        "import docx\n",
        "import re\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "import textwrap\n",
        "from google.colab import files"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9ETeEqDOG8qI"
      },
      "outputs": [],
      "source": [
        "# set up the model\n",
        "genai.configure(api_key=\"AIzaSyCr7B-Ox86HFFDqodh2p2pruSTrmgN9MD4\")\n",
        "\n",
        "generation_config ={\n",
        "    \"temperature\": 0.5,\n",
        "    \"max_output_tokens\": 8192,\n",
        "    \"top_k\": 40,\n",
        "    \"top_p\": 0.95\n",
        "}\n",
        "\n",
        "\n",
        "# adding safety features\n",
        "safety_settings = [\n",
        "    {\"category\": \"HARM_CATEGORY_HARASSMENT\", \"threshold\": \"BLOCK_NONE\"},\n",
        "    {\"category\": \"HARM_CATEGORY_HATE_SPEECH\", \"threshold\": \"BLOCK_NONE\"},\n",
        "    {\"category\": \"HARM_CATEGORY_SEXUALLY_EXPLICIT\", \"threshold\": \"BLOCK_NONE\"},\n",
        "    {\"category\": \"HARM_CATEGORY_DANGEROUS_CONTENT\", \"threshold\": \"BLOCK_NONE\"},\n",
        "]\n",
        "\n",
        "model = genai.GenerativeModel(\n",
        "    'gemini-2.0-flash-001',\n",
        "    generation_config=generation_config,\n",
        "    safety_settings=safety_settings\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CwL45ZrvHLGY"
      },
      "outputs": [],
      "source": [
        "# Function to extract the text from the uploaded resume, include pdf, word or text format\n",
        "\n",
        "def extract_text_from_pdf(pdf_path):\n",
        "    text = \"\"\n",
        "    with open(pdf_path, 'rb') as file:\n",
        "        pdf_reader = PyPDF2.PdfReader(file)\n",
        "        for page in pdf_reader.pages:\n",
        "            text += page.extract_text()\n",
        "    return text\n",
        "\n",
        "def extract_text_from_docx(docx_path):\n",
        "    doc = docx.Document(docx_path)\n",
        "    full_text = []\n",
        "    for para in doc.paragraphs:\n",
        "        full_text.append(para.text)\n",
        "    return '\\n'.join(full_text)\n",
        "\n",
        "# To extract different versions of the resume\n",
        "def extract_text(file_path):\n",
        "    if file_path.endswith('.pdf'):\n",
        "        return extract_text_from_pdf(file_path)\n",
        "    elif file_path.endswith('.docx'):\n",
        "        return extract_text_from_docx(file_path)\n",
        "    else:\n",
        "        with open(file_path, 'r') as file:\n",
        "            return file.read()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1SLH5RSlHbd9"
      },
      "outputs": [],
      "source": [
        "# Using embedding to make sure the text fits the model\n",
        "def get_embedding(text):\n",
        "    truncated_text = text[:10000]\n",
        "    result = genai.embed_content(\n",
        "        model=\"models/embedding-001\",\n",
        "        content=truncated_text,\n",
        "        task_type=\"retrieval_document\"\n",
        "    )\n",
        "    return result['embedding']\n",
        "\n",
        "# use cosine to calculate similarity between two texts\n",
        "def calculate_similarity(text1, text2):\n",
        "    emb1 = get_embedding(text1)\n",
        "    emb2 = get_embedding(text2)\n",
        "    return cosine_similarity([emb1], [emb2])[0][0]\n",
        "\n",
        "\n",
        "\n",
        "# analyse and refine the resume\n",
        "# Passing the prompt to Returns structured analysis with: match score, strengths, gaps, and recommendations\n",
        "def analyze_alignment(resume_text, jd_text):\n",
        "\n",
        "    prompt = textwrap.dedent(f\"\"\"\n",
        "    **Resume-Job Description Alignment Analysis**\n",
        "\n",
        "    Analyze how well this resume matches the job description by examining:\n",
        "    - Required skills/technologies\n",
        "    - Years and type of experience\n",
        "    - Education/certifications\n",
        "    - Keywords and terminology\n",
        "    - Soft skills and cultural fit\n",
        "\n",
        "    **Resume Content:**\n",
        "    {resume_text[:10000]}  # Limiting to first 10k chars for Gemini context\n",
        "\n",
        "    **Job Description Content:**\n",
        "    {jd_text[:10000]}\n",
        "\n",
        "    Provide your analysis in this exact format:\n",
        "\n",
        "    [Explanation of overall match qualities]\n",
        "\n",
        "    ### Key Strengths:\n",
        "    - [Strength 1 with specific evidence] from resume\n",
        "    - [Strength 2 with specific evidence]\n",
        "    - [Strength 3 with specific evidence]\n",
        "\n",
        "    ### Critical Gaps:\n",
        "    - [Missing requirement 1 with impact] (Critical/Important/Nice-to-Have)\n",
        "    - [Missing requirement 2 with impact]\n",
        "    - [Missing requirement 3 with impact]\n",
        "\n",
        "    ### Improvement Recommendations:\n",
        "    - [Actionable suggestion 1] (most impactful)\n",
        "    - [Actionable suggestion 2]\n",
        "    - [Actionable suggestion 3]\n",
        "\n",
        "    ### Keyword Optimization:\n",
        "    [List of important JD keywords missing from resume]\n",
        "    \"\"\")\n",
        "\n",
        "    # Error catch\n",
        "    try:\n",
        "        response = model.generate_content(prompt)\n",
        "        return response.text\n",
        "    except Exception as e:\n",
        "        return f\"Analysis failed: {str(e)}\"\n",
        "\n",
        "# Refine the resume to match the job description.\n",
        "# This Maintains original information while optimizing for the JD\n",
        "\n",
        "def refine_resume(resume_text, jd_text):\n",
        "\n",
        "    \"\"\"Responsible resume refinement with compatibility check\"\"\"\n",
        "    # First check compatibility\n",
        "    compat = calculate_compatibility(resume_text, jd_text)\n",
        "\n",
        "    if compat['score'] < 40:  # Threshold for poor match\n",
        "        return {\n",
        "            'status': 'poor_match',\n",
        "            'message': f\"Low compatibility ({compat['score']}%) {compat['reason']}\",\n",
        "            'suggested_roles': compat['suggested_roles'],\n",
        "            'refined_resume': None\n",
        "        }\n",
        "\n",
        "    prompt = textwrap.dedent(f\"\"\"\n",
        "    **Resume Refinement Task**\n",
        "\n",
        "    Carefully refine this resume for the target job:\n",
        "\n",
        "\n",
        "    You are a professional resume writer. Rewrite this resume to better match the\n",
        "    target job description while:\n",
        "    1. Preserve all factual information\n",
        "    2. Only add skills/experiences that can be reasonably inferred\n",
        "    3. Never falsify qualifications\n",
        "    4. Prioritize keywords from the job description\n",
        "    5. Keep original formatting style and structure\n",
        "    6. Reordering content to highlight relevant experience first\n",
        "    7. Quantifying achievements where possible\n",
        "\n",
        "    **Original Resume:**\n",
        "    {resume_text[:10000]}\n",
        "\n",
        "    **Target Job Description:**\n",
        "    {jd_text[:10000]}\n",
        "\n",
        "    **MODIFICATION GUIDELINES:**\n",
        "    - Incorporate at least 3 key terms from the JD\n",
        "    - Combine skills to a 'Relevant Skills' section, limit the number of skills to the resonable for a resume\n",
        "    - Ensure work experience highlights JD-relevant accomplishments\n",
        "    - Optimize the professional summary for this role\n",
        "\n",
        "    Return ONLY the refined resume text with no additional commentary or analysis.\n",
        "    Maintain the same format as the original (bullet points, sections, etc).\n",
        "    \"\"\")\n",
        "\n",
        "# Error catch\n",
        "\n",
        "    try:\n",
        "        response = model.generate_content(prompt)\n",
        "\n",
        "        # Clean up response to ensure only resume content\n",
        "        refined = response.text\n",
        "        if refined.startswith(\"```\"):\n",
        "            refined = refined[:-3]\n",
        "        return {\n",
        "            'status': 'success',\n",
        "            'compatibility_score': compat['score'],\n",
        "            'refined_resume': refined.strip(),\n",
        "            'analysis': compat['reason']\n",
        "        }\n",
        "\n",
        "    except Exception as e:\n",
        "        # Print or log the error\n",
        "        print(f\"Refinement failed: {str(e)}\")  # Print or log the error\n",
        "\n",
        "\n",
        "        # Return a dictionary indicating failure\n",
        "        return {\n",
        "            'status': 'error',\n",
        "            'message': f\"Refinement failed: {str(e)}\",\n",
        "            'refined_resume': None\n",
        "        }"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HkeNlcwSHVhr"
      },
      "outputs": [],
      "source": [
        "def calculate_compatibility(resume_text, jd_text):\n",
        "    \"\"\"Returns compatibility score and reason\"\"\"\n",
        "    prompt = f\"\"\"\n",
        "    Analyze compatibility between this resume and job description:\n",
        "\n",
        "    RESUME:\n",
        "    {resume_text[:10000]}\n",
        "\n",
        "    JOB DESCRIPTION:\n",
        "    {jd_text[:10000]}\n",
        "\n",
        "    Respond STRICTLY in this format:\n",
        "\n",
        "    COMPATIBILITY_SCORE: [0-100]%\n",
        "    REASON: [concise mismatch explanation]\n",
        "    SUGGGESTED_ROLES: [Role1] | [Role2] | [Role3]\n",
        "\n",
        "    Example:\n",
        "    COMPATIBILITY_SCORE: 5%\n",
        "    REASON: The resume is for an HR Generalist while the job description is for a Data Analyst.\n",
        "    SUGGGESTED_ROLES: HR Data Specialist | People Analytics | Recruitment Coordinator\n",
        "    \"\"\"\n",
        "    response = model.generate_content(prompt)\n",
        "    return parse_compatibility_response(response.text)\n",
        "\n",
        "\n",
        "def parse_compatibility_response(text):\n",
        "    \"\"\"More robust parsing\"\"\"\n",
        "    result = {\n",
        "        'score': 0,\n",
        "        'reason': \"No analysis available\",\n",
        "        'suggested_roles': [\"HR Data Specialist\", \"People Analytics\", \"Recruitment Coordinator\"]  # Defaults\n",
        "    }\n",
        "\n",
        "    # Extract score\n",
        "    score_match = re.search(r\"COMPATIBILITY_SCORE:\\s*(\\d+)%\", text)\n",
        "    if score_match:\n",
        "        result['score'] = int(score_match.group(1))\n",
        "\n",
        "    # Extract reason\n",
        "    reason_match = re.search(r\"REASON:\\s*(.+?)(?:\\n|SUGGESTED_ROLES:|$)\", text, re.DOTALL)\n",
        "    if reason_match:\n",
        "        result['reason'] = reason_match.group(1).strip()\n",
        "\n",
        "    # Extract roles (handles various delimiters)\n",
        "    roles_match = re.search(r\"SUGGESTED_ROLES:\\s*(.+)\", text)\n",
        "    if roles_match:\n",
        "        roles = roles_match.group(1)\n",
        "        result['suggested_roles'] = [r.strip() for r in re.split(r\"\\||,|;\", roles) if r.strip()]\n",
        "\n",
        "    return result\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FkBTtx9NHx87"
      },
      "outputs": [],
      "source": [
        "def generate_cover_letter(refined_resume, jd_text):\n",
        "    \"\"\"Creates tailored cover letter\"\"\"\n",
        "    prompt = f\"\"\"\n",
        "    Write a professional cover letter combining:\n",
        "\n",
        "    CANDIDATE QUALIFICATIONS:\n",
        "    {refined_resume[:10000]}\n",
        "\n",
        "    JOB REQUIREMENTS:\n",
        "    {jd_text[:10000]}\n",
        "\n",
        "    Guidelines:\n",
        "    1. Make sure to include know information e.g name, address, phone number etc from the resume\n",
        "    2. 3-4 concise paragraphs\n",
        "    3. Highlight 3 key matching qualifications\n",
        "    4. Address potential gaps diplomatically\n",
        "    5. Express interest in the position and company\n",
        "    6. Include a passionate closing statement\n",
        "\n",
        "    Return ONLY the cover letter text.\n",
        "    \"\"\"\n",
        "    return model.generate_content(prompt).text"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4b4GZC6bH5d8"
      },
      "outputs": [],
      "source": [
        "# Main function\n",
        "def main():\n",
        "    print(\" Resume Analysis & Optimization Tool\")\n",
        "    print(\"-------------------------------------\")\n",
        "\n",
        "    # File uploads\n",
        "    print(\"\\n STEP 1: Upload your resume (PDF/DOCX/TXT):\")\n",
        "    uploaded_resume = files.upload()\n",
        "    resume_file = next(iter(uploaded_resume))\n",
        "    resume_text = extract_text(resume_file)\n",
        "\n",
        "    print(\"\\n STEP 2: Upload the job description (PDF/DOCX/TXT):\")\n",
        "    uploaded_jd = files.upload()\n",
        "    jd_file = next(iter(uploaded_jd))\n",
        "    jd_text = extract_text(jd_file)\n",
        "\n",
        "    # Calculate embedding similarity\n",
        "    print(\"\\n Analyzing documents...\")\n",
        "    similarity_score = calculate_similarity(resume_text, jd_text)\n",
        "    print(f\"\\n Semantic Match Score: {similarity_score*100:.1f}% \\n\")\n",
        "\n",
        "  # calling the refine resume funct\n",
        "    optimized_resume = refine_resume(resume_text, jd_text)\n",
        "\n",
        "    if optimized_resume['status'] == 'poor_match':\n",
        "      print(f\"\\n‚ö†Ô∏è Compatibility Alert: \\n {optimized_resume['message']}\\n\")\n",
        "      if optimized_resume['suggested_roles']:\n",
        "        print(\"\\n Suggested Alternative Roles:\")\n",
        "        for i, role in enumerate(optimized_resume['suggested_roles'][:3], 1):\n",
        "            print(f\"{i}. {role}\")\n",
        "      return\n",
        "\n",
        "    else:  # Detailed analysis\n",
        "      print(f\" Compatibility Score: {optimized_resume['compatibility_score']}% \\n\")\n",
        "      print(\"\\n Detailed Analysis:\")\n",
        "      analysis = analyze_alignment(resume_text, jd_text)\n",
        "      print(analysis)\n",
        "\n",
        "    # Generate cover letter for compatible matches\n",
        "      cover_letter = generate_cover_letter(\n",
        "          optimized_resume['refined_resume'],\n",
        "          jd_text\n",
        "      )\n",
        "\n",
        "      # Resume refinement\n",
        "      print(\"\\n Generating optimized resume...\")\n",
        "      print(\"\\n OPTIMIZED RESUME:\")\n",
        "      print(\"-------------------\")\n",
        "      print(optimized_resume['refined_resume'])\n",
        "      print(\"\\n Generating COVER LETTER...\")\n",
        "      print(\"\\n COVER LETTER:\")\n",
        "      print(\"--------------\")\n",
        "      print(cover_letter)\n",
        "\n",
        "      # Save and offer download\n",
        "      with open('optimized_resume.txt', 'w') as f:\n",
        "        f.write(optimized_resume['refined_resume'])\n",
        "      print(\"\\n Downloaded your optimized resume:\")\n",
        "      files.download('optimized_resume.txt')\n",
        "\n",
        "      with open('cover_letter.txt', 'w') as f:\n",
        "        f.write(cover_letter)\n",
        "      print(\"\\n Downloaded your cover letter:\")\n",
        "      files.download('cover_letter.txt')\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n",
        "\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}